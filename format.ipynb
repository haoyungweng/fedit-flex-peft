{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1744868286225,
     "user": {
      "displayName": "Wang Rao",
      "userId": "02491972522068568822"
     },
     "user_tz": 240
    },
    "id": "149OwE0WkJCQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def print_tex_result(results_file, primary_metrics, percent_metrics):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    task_metrics = {}\n",
    "    for task, metric_name in primary_metrics.items():\n",
    "        if task in results and metric_name in results[task]:\n",
    "            value = results[task][metric_name]\n",
    "            if metric_name in percent_metrics:\n",
    "                task_metrics[task] = value * 100\n",
    "            else:\n",
    "                task_metrics[task] = value\n",
    "        else:\n",
    "            task_metrics[task] = np.nan\n",
    "    return task_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744868633631,
     "user": {
      "displayName": "Wang Rao",
      "userId": "02491972522068568822"
     },
     "user_tz": 240
    },
    "id": "2VPQE_zsLXFT",
    "outputId": "e92e4ada-3e4c-40a2-ce85-54abe86870a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: hetero\n",
      "59.00 & 65.50 & 75.50 & 68.00 & 86.00 & 51.48 & 92.92 & 62.00 \\\\\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"evaluations_final/hetero-3B-final-468/20\"\n",
    "mode = \"hetero\"  # Change this to \"none\", \"homo\", or \"hetero\"\n",
    "num_clients = 8 # Define the number of clients\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"accuracy\",\n",
    "    \"entailment\": \"accuracy\",\n",
    "    \"linguistic_acceptability\": \"accuracy\",\n",
    "    \"paraphrase\": \"accuracy\",\n",
    "    \"question_classification\": \"accuracy\",\n",
    "    \"structure_to_text\": \"rougeL\",\n",
    "    \"text_formatting\": \"rougeL\",\n",
    "    \"word_disambiguation\": \"accuracy\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "if mode == \"homo\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    try:\n",
    "        metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        latex_row = \" & \".join([f\"{metrics.get(task, np.nan):.2f}\" for task in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Results file not found at {results_path}\")\n",
    "\n",
    "elif mode == \"none\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    all_rows = []\n",
    "    # 1) load everything\n",
    "    for i in range(num_clients):\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        try:\n",
    "            task_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "            all_rows.append([task_metrics.get(task, np.nan) for task in latex_order])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Results file not found for client {i} at {results_path}. Skipping client.\")\n",
    "            all_rows.append([np.nan] * len(latex_order)) # Add row of NaNs\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"Error: No client data loaded.\")\n",
    "    else:\n",
    "        all_array   = np.array(all_rows)\n",
    "        best_values = np.nanmax(all_array, axis=0)\n",
    "        avg_values  = np.nanmean(all_array, axis=0)\n",
    "\n",
    "        # 2) print per‐client rows, bolding only the column‐wise maxima\n",
    "        for i, row_vals in enumerate(all_rows):\n",
    "            cells = []\n",
    "            for idx, val in enumerate(row_vals):\n",
    "                # Check if val is NaN before comparison\n",
    "                if not np.isnan(val) and np.isclose(val, best_values[idx]):\n",
    "                    cells.append(f\"\\\\textbf{{{val:.2f}}}\")\n",
    "                elif np.isnan(val):\n",
    "                     cells.append(f\"NaN\") # Or some other placeholder\n",
    "                else:\n",
    "                    cells.append(f\"{val:.2f}\")\n",
    "            print(f\"{i} & \" + \" & \".join(cells) + \" \\\\\\\\\")\n",
    "\n",
    "        # 3) average row\n",
    "        print('\\\\cline{2-10}') # Adjust column range if needed\n",
    "        avg_cells = [f\"{v:.2f}\" if not np.isnan(v) else \"NaN\" for v in avg_values]\n",
    "        print(f\"none (avg) & \" + \" & \".join(avg_cells) + \" \\\\\\\\\")\n",
    "\n",
    "        # 4) best row (still bold)\n",
    "        best_cells = [f\"{v:.2f}\" if not np.isnan(v) else \"NaN\" for v in best_values]\n",
    "        print(f\"none (best) & \" + \" & \".join(best_cells) + \" \\\\\\\\\")\n",
    "\n",
    "elif mode == \"hetero\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    hetero_metrics = {}\n",
    "    valid_clients_count = 0\n",
    "    # Load metrics for each client, assuming client i corresponds to task i in latex_order\n",
    "    # This interpretation might need adjustment based on your exact setup.\n",
    "    if len(latex_order) != num_clients:\n",
    "         print(f\"Warning: Number of tasks ({len(latex_order)}) does not match number of clients ({num_clients}). Diagonal logic might be incorrect.\")\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        task = latex_order[i] # Task corresponding to this client index\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_hetlora_output_metrics.json\")\n",
    "        try:\n",
    "            client_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "            # Get the primary metric for the specific task 'task' from this client's results\n",
    "            hetero_metrics[task] = client_metrics.get(task, np.nan)\n",
    "            if not np.isnan(hetero_metrics[task]):\n",
    "                 valid_clients_count += 1\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Results file not found for client {i} at {results_path}. Setting metric for task '{task}' to NaN.\")\n",
    "            hetero_metrics[task] = np.nan\n",
    "\n",
    "    if valid_clients_count == 0:\n",
    "        print(\"Error: No HetLoRA client data loaded.\")\n",
    "    else:\n",
    "        # Format the 'diagonal' metrics into a single row, similar to 'homo' mode\n",
    "        latex_row = \" & \".join([f\"{hetero_metrics.get(t, np.nan):.2f}\" for t in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Invalid mode '{mode}'. Choose 'none', 'homo', or 'hetero'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 & \\textbf{54.50}/55.50 & 34.00/35.50 & \\textbf{64.50}/50.50 & 58.30/55.76 & 30.50/35.00 & 29.68/37.38 & 68.73/85.39 & 47.00/50.50 \\\\\n",
      "1 & 53.00/56.00 & \\textbf{35.00}/37.50 & \\textbf{64.50}/41.50 & 68.94/67.94 & 29.00/36.50 & 29.09/36.89 & 72.77/86.59 & 49.50/50.50 \\\\\n",
      "2 & 53.50/\\textbf{57.50} & 34.50/43.00 & 64.00/\\textbf{57.50} & \\textbf{75.33}/\\textbf{75.00} & 28.00/40.00 & 30.76/35.33 & 69.63/85.85 & 49.50/\\textbf{51.00} \\\\\n",
      "3 & 53.50/55.00 & 34.50/36.50 & \\textbf{64.50}/49.50 & 23.75/47.52 & 32.50/37.00 & 28.69/35.55 & 72.76/85.72 & 47.00/\\textbf{51.00} \\\\\n",
      "4 & 51.50/53.50 & 34.50/\\textbf{43.50} & \\textbf{64.50}/49.50 & 60.41/70.04 & \\textbf{39.00}/\\textbf{45.00} & 29.68/36.08 & 68.10/84.64 & 47.00/\\textbf{51.00} \\\\\n",
      "5 & 50.00/34.50 & 28.50/0.00 & 44.00/0.00 & 14.15/0.00 & 11.00/0.50 & \\textbf{33.84}/\\textbf{43.04} & 72.56/85.34 & 15.00/10.00 \\\\\n",
      "6 & 46.00/36.50 & 23.50/0.00 & 33.00/0.00 & 12.21/2.96 & 14.00/2.00 & 33.54/39.66 & \\textbf{89.18}/\\textbf{90.77} & 12.00/17.00 \\\\\n",
      "7 & 54.00/54.00 & 34.50/39.50 & \\textbf{64.50}/53.50 & 70.41/72.08 & 30.50/39.00 & 29.25/36.41 & 73.04/85.19 & \\textbf{51.00}/50.50 \\\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Two result directories to compare\n",
    "results_dir1 = \"evaluations_midway/none-1B-midway/20\"\n",
    "results_dir2 = \"evaluations_midway/none-3B-midway/20\"\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"accuracy\",\n",
    "    \"entailment\": \"accuracy\",\n",
    "    \"linguistic_acceptability\": \"accuracy\",\n",
    "    \"paraphrase\": \"accuracy\",\n",
    "    \"question_classification\": \"accuracy\",\n",
    "    \"structure_to_text\": \"rougeL\",\n",
    "    \"text_formatting\": \"rougeL\",\n",
    "    \"word_disambiguation\": \"accuracy\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "# 1) Gather all client scores for each directory\n",
    "all_rows1 = []\n",
    "all_rows2 = []\n",
    "for i in range(8):\n",
    "    path1 = os.path.join(results_dir1, f\"client_{i}_output_metrics.json\")\n",
    "    path2 = os.path.join(results_dir2, f\"client_{i}_output_metrics.json\")\n",
    "\n",
    "    m1 = print_tex_result(path1, primary_metrics, percent_metrics)\n",
    "    m2 = print_tex_result(path2, primary_metrics, percent_metrics)\n",
    "\n",
    "    all_rows1.append([m1[task] for task in latex_order])\n",
    "    all_rows2.append([m2[task] for task in latex_order])\n",
    "\n",
    "all_array1 = np.array(all_rows1)\n",
    "all_array2 = np.array(all_rows2)\n",
    "\n",
    "# 2) Compute per-column bests for each directory separately\n",
    "best1 = np.nanmax(all_array1, axis=0)\n",
    "best2 = np.nanmax(all_array2, axis=0)\n",
    "\n",
    "# 3) Print each client row, bolding entries that match the best within their own dir\n",
    "for i in range(8):\n",
    "    cells = []\n",
    "    for idx, task in enumerate(latex_order):\n",
    "        v1 = all_rows1[i][idx]\n",
    "        v2 = all_rows2[i][idx]\n",
    "\n",
    "        # bold if this client has the max for that column in its directory\n",
    "        if np.isclose(v1, best1[idx]):\n",
    "            s1 = f\"\\\\textbf{{{v1:.2f}}}\"\n",
    "        else:\n",
    "            s1 = f\"{v1:.2f}\"\n",
    "\n",
    "        if np.isclose(v2, best2[idx]):\n",
    "            s2 = f\"\\\\textbf{{{v2:.2f}}}\"\n",
    "        else:\n",
    "            s2 = f\"{v2:.2f}\"\n",
    "\n",
    "        cells.append(f\"{s1}/{s2}\")\n",
    "\n",
    "    row_str = \" & \".join(cells)\n",
    "    print(f\"{i} & {row_str} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.23 & 35.00 & 60.00 & 57.50 & 73.50 & 52.11 & 82.29 & 50.50 & 59.02 \\\\\n"
     ]
    }
   ],
   "source": [
    "# for appendix A\n",
    "\n",
    "results_dir = \"evaluations_midway/hetero-3B-midway/20\"\n",
    "is_global_model = True\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"rouge1\",\n",
    "    \"entailment\": \"rouge1\",\n",
    "    \"linguistic_acceptability\": \"rouge1\",\n",
    "    \"paraphrase\": \"rouge1\",\n",
    "    \"question_classification\": \"rouge1\",\n",
    "    \"structure_to_text\": \"rouge1\",\n",
    "    \"text_formatting\": \"rouge1\",\n",
    "    \"word_disambiguation\": \"rouge1\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\", \"rouge1\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "if is_global_model:\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "    latex_row = \" & \".join([f\"{metrics[task]:.2f}\" for task in latex_order]) + \" & \" + f\"{sum(metrics.values()) / len(metrics):.2f}\" + \" \\\\\\\\\"\n",
    "    print(latex_row)\n",
    "\n",
    "else:\n",
    "    all_rows = []\n",
    "    # 1) load everything\n",
    "    for i in range(8):\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        task_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        all_rows.append([task_metrics[task] for task in latex_order])\n",
    "\n",
    "    all_array   = np.array(all_rows)\n",
    "    best_values = np.nanmax(all_array, axis=0)\n",
    "    avg_values  = np.nanmean(all_array, axis=0)\n",
    "\n",
    "    # 2) print per‐client rows, bolding only the column‐wise maxima\n",
    "    for i, row_vals in enumerate(all_rows):\n",
    "        cells = []\n",
    "        for val, best in zip(row_vals, best_values):\n",
    "            if np.isclose(val, best):\n",
    "                cells.append(f\"\\\\textbf{{{val:.2f}}}\")\n",
    "            else:\n",
    "                cells.append(f\"{val:.2f}\")\n",
    "        print(f\"& {i} & \" + \" & \".join(cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 3) average row\n",
    "    print('\\\\cline{2-10}')\n",
    "    avg_cells = [f\"{v:.2f}\" for v in avg_values]\n",
    "    print(f\"& none (avg) & \" + \" & \".join(avg_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 4) best row (still bold)\n",
    "    best_cells = [f\"{v:.2f}\" for v in best_values]\n",
    "    print(f\"& none (best) & \" + \" & \".join(best_cells) + \" \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODRrMkgL9BwBuSuCHRTfMB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "HFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
