{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1744868286225,
     "user": {
      "displayName": "Wang Rao",
      "userId": "02491972522068568822"
     },
     "user_tz": 240
    },
    "id": "149OwE0WkJCQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def print_tex_result(results_file, primary_metrics, percent_metrics):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    task_metrics = {}\n",
    "    for task, metric_name in primary_metrics.items():\n",
    "        if task in results and metric_name in results[task]:\n",
    "            value = results[task][metric_name]\n",
    "            if metric_name in percent_metrics:\n",
    "                task_metrics[task] = value * 100\n",
    "            else:\n",
    "                task_metrics[task] = value\n",
    "        else:\n",
    "            task_metrics[task] = np.nan\n",
    "    return task_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"evaluations_final/hetero-3B-optimal-426/20/\"\n",
    "mode = \"hetero-p\"  # Change this to \"none\", \"homo\", \"hetero-g\", \"hetero-d\" or \"hetero-p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744868633631,
     "user": {
      "displayName": "Wang Rao",
      "userId": "02491972522068568822"
     },
     "user_tz": 240
    },
    "id": "2VPQE_zsLXFT",
    "outputId": "e92e4ada-3e4c-40a2-ce85-54abe86870a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: hetero-p\n",
      "57.00 & 76.50 & 63.50 & 57.50 & 85.50 & 51.98 & 94.65 & 56.50 \\\\\n"
     ]
    }
   ],
   "source": [
    "num_clients = 8 # Define the number of clients\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"accuracy\",\n",
    "    \"entailment\": \"accuracy\",\n",
    "    \"linguistic_acceptability\": \"accuracy\",\n",
    "    \"paraphrase\": \"accuracy\",\n",
    "    \"question_classification\": \"accuracy\",\n",
    "    \"structure_to_text\": \"rougeL\",\n",
    "    \"text_formatting\": \"rougeL\",\n",
    "    \"word_disambiguation\": \"accuracy\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "if mode == \"homo\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    try:\n",
    "        metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        latex_row = \" & \".join([f\"{metrics.get(task, np.nan):.2f}\" for task in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Results file not found at {results_path}\")\n",
    "\n",
    "elif mode == \"none\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    all_rows = []\n",
    "    # 1) load everything\n",
    "    for i in range(num_clients):\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        try:\n",
    "            task_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "            all_rows.append([task_metrics.get(task, np.nan) for task in latex_order])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Results file not found for client {i} at {results_path}. Skipping client.\")\n",
    "            all_rows.append([np.nan] * len(latex_order)) # Add row of NaNs\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"Error: No client data loaded.\")\n",
    "    else:\n",
    "        all_array   = np.array(all_rows)\n",
    "        best_values = np.nanmax(all_array, axis=0)\n",
    "        avg_values  = np.nanmean(all_array, axis=0)\n",
    "\n",
    "        # 2) print per‐client rows, bolding only the column‐wise maxima\n",
    "        for i, row_vals in enumerate(all_rows):\n",
    "            cells = []\n",
    "            for idx, val in enumerate(row_vals):\n",
    "                # Check if val is NaN before comparison\n",
    "                if not np.isnan(val) and np.isclose(val, best_values[idx]):\n",
    "                    cells.append(f\"\\\\textbf{{{val:.2f}}}\")\n",
    "                elif np.isnan(val):\n",
    "                     cells.append(f\"NaN\") # Or some other placeholder\n",
    "                else:\n",
    "                    cells.append(f\"{val:.2f}\")\n",
    "            print(f\"{i} & \" + \" & \".join(cells) + \" \\\\\\\\\")\n",
    "\n",
    "        # 3) average row\n",
    "        print('\\\\cline{2-10}') # Adjust column range if needed\n",
    "        avg_cells = [f\"{v:.2f}\" if not np.isnan(v) else \"NaN\" for v in avg_values]\n",
    "        print(f\"none (avg) & \" + \" & \".join(avg_cells) + \" \\\\\\\\\")\n",
    "\n",
    "        # 4) best row (still bold)\n",
    "        best_cells = [f\"{v:.2f}\" if not np.isnan(v) else \"NaN\" for v in best_values]\n",
    "        print(f\"none (best) & \" + \" & \".join(best_cells) + \" \\\\\\\\\")\n",
    "\n",
    "elif mode == \"hetero-g\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    try:\n",
    "        metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        latex_row = \" & \".join([f\"{metrics.get(task, np.nan):.2f}\" for task in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Results file not found at {results_path}\")\n",
    "\n",
    "elif mode == \"hetero-d\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    hetero_metrics = {}\n",
    "    valid_clients_count = 0\n",
    "    # Load metrics for each client, assuming client i corresponds to task i in latex_order\n",
    "    # This interpretation might need adjustment based on your exact setup.\n",
    "    if len(latex_order) != num_clients:\n",
    "         print(f\"Warning: Number of tasks ({len(latex_order)}) does not match number of clients ({num_clients}). Diagonal logic might be incorrect.\")\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        task = latex_order[i] # Task corresponding to this client index\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_hetlora_output_metrics.json\")\n",
    "        try:\n",
    "            client_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "            # Get the primary metric for the specific task 'task' from this client's results\n",
    "            hetero_metrics[task] = client_metrics.get(task, np.nan)\n",
    "            if not np.isnan(hetero_metrics[task]):\n",
    "                 valid_clients_count += 1\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Results file not found for client {i} at {results_path}. Setting metric for task '{task}' to NaN.\")\n",
    "            hetero_metrics[task] = np.nan\n",
    "\n",
    "    if valid_clients_count == 0:\n",
    "        print(\"Error: No HetLoRA client data loaded.\")\n",
    "    else:\n",
    "        # Format the 'diagonal' metrics into a single row, similar to 'homo' mode\n",
    "        latex_row = \" & \".join([f\"{hetero_metrics.get(t, np.nan):.2f}\" for t in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "\n",
    "elif mode == \"hetero-p\":\n",
    "    print(f\"Mode: {mode}\")\n",
    "    hetero_metrics = {}\n",
    "    valid_clients_count = 0\n",
    "    # Load metrics for each client, assuming client i corresponds to task i in latex_order\n",
    "    # This interpretation might need adjustment based on your exact setup.\n",
    "    if len(latex_order) != num_clients:\n",
    "         print(f\"Warning: Number of tasks ({len(latex_order)}) does not match number of clients ({num_clients}). Diagonal logic might be incorrect.\")\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        task = latex_order[i] # Task corresponding to this client index\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        try:\n",
    "            client_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "            # Get the primary metric for the specific task 'task' from this client's results\n",
    "            hetero_metrics[task] = client_metrics.get(task, np.nan)\n",
    "            if not np.isnan(hetero_metrics[task]):\n",
    "                 valid_clients_count += 1\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Results file not found for client {i} at {results_path}. Setting metric for task '{task}' to NaN.\")\n",
    "            hetero_metrics[task] = np.nan\n",
    "\n",
    "    if valid_clients_count == 0:\n",
    "        print(\"Error: No HetLoRA client data loaded.\")\n",
    "    else:\n",
    "        # Format the 'diagonal' metrics into a single row, similar to 'homo' mode\n",
    "        latex_row = \" & \".join([f\"{hetero_metrics.get(t, np.nan):.2f}\" for t in latex_order]) + \" \\\\\\\\\"\n",
    "        print(latex_row)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Invalid mode '{mode}'. Choose 'none', 'homo', or 'hetero'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 & \\textbf{53.00}/\\textbf{73.00} & 37.00/32.50 & 62.00/65.00 & 48.00/41.00 & 17.50/57.00 & 28.33/37.47 & 31.36/75.88 & 48.00/53.00 \\\\\n",
      "1 & 52.00/55.50 & \\textbf{76.00}/\\textbf{86.50} & 61.50/64.50 & 35.50/38.50 & 6.50/53.00 & 29.81/32.90 & 44.38/71.46 & 42.00/48.50 \\\\\n",
      "2 & \\textbf{53.00}/62.00 & 38.50/52.50 & 51.50/\\textbf{78.50} & 53.00/44.00 & 30.00/43.50 & 26.02/34.22 & 42.14/80.94 & 33.00/38.00 \\\\\n",
      "3 & 45.00/55.00 & 34.50/48.50 & 57.00/65.50 & \\textbf{69.50}/\\textbf{70.00} & 21.50/44.50 & 25.68/36.78 & 40.09/77.83 & 28.50/55.00 \\\\\n",
      "4 & \\textbf{53.00}/56.00 & 36.00/40.50 & \\textbf{64.50}/65.00 & 38.00/37.00 & \\textbf{86.00}/\\textbf{91.50} & 21.70/34.51 & 43.09/77.93 & 38.00/48.50 \\\\\n",
      "5 & 3.50/23.00 & 0.50/23.00 & 0.00/24.00 & 0.50/7.00 & 0.00/16.00 & \\textbf{51.08}/\\textbf{53.84} & 61.16/75.13 & 0.50/21.00 \\\\\n",
      "6 & 0.00/4.50 & 0.00/13.00 & 0.00/14.50 & 0.00/12.00 & 0.00/22.00 & 40.76/41.18 & \\textbf{93.70}/\\textbf{96.31} & 0.00/35.00 \\\\\n",
      "7 & 50.00/53.00 & 37.00/36.00 & 60.50/61.00 & 40.00/48.00 & 25.50/42.00 & 29.22/37.07 & 52.93/77.57 & \\textbf{53.50}/\\textbf{56.00} \\\\\n",
      "\\hline\n",
      "Avg & 38.69/47.75 & 32.44/41.56 & 44.62/54.75 & 35.56/37.19 & 23.38/46.19 & 31.57/38.50 & 51.11/79.13 & 30.44/44.38 \\\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Two result directories to compare\n",
    "results_dir1 = \"evaluations_final/none-homo-1B-426/20\"\n",
    "results_dir2 = \"evaluations_final/none-homo-3B-426/20\"\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"accuracy\",\n",
    "    \"entailment\": \"accuracy\",\n",
    "    \"linguistic_acceptability\": \"accuracy\",\n",
    "    \"paraphrase\": \"accuracy\",\n",
    "    \"question_classification\": \"accuracy\",\n",
    "    \"structure_to_text\": \"rougeL\",\n",
    "    \"text_formatting\": \"rougeL\",\n",
    "    \"word_disambiguation\": \"accuracy\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "# 1) Gather all client scores for each directory\n",
    "all_rows1 = []\n",
    "all_rows2 = []\n",
    "for i in range(8):\n",
    "    path1 = os.path.join(results_dir1, f\"client_{i}_output_metrics.json\")\n",
    "    path2 = os.path.join(results_dir2, f\"client_{i}_output_metrics.json\")\n",
    "\n",
    "    m1 = print_tex_result(path1, primary_metrics, percent_metrics)\n",
    "    m2 = print_tex_result(path2, primary_metrics, percent_metrics)\n",
    "\n",
    "    all_rows1.append([m1[task] for task in latex_order])\n",
    "    all_rows2.append([m2[task] for task in latex_order])\n",
    "\n",
    "all_array1 = np.array(all_rows1)\n",
    "all_array2 = np.array(all_rows2)\n",
    "\n",
    "# 2) Compute per-column bests for each directory separately\n",
    "best1 = np.nanmax(all_array1, axis=0)\n",
    "best2 = np.nanmax(all_array2, axis=0)\n",
    "\n",
    "# 3) Print each client row, bolding entries that match the best within their own dir\n",
    "for i in range(8):\n",
    "    cells = []\n",
    "    for idx, task in enumerate(latex_order):\n",
    "        v1 = all_rows1[i][idx]\n",
    "        v2 = all_rows2[i][idx]\n",
    "\n",
    "        # bold if this client has the max for that column in its directory\n",
    "        if np.isclose(v1, best1[idx]):\n",
    "            s1 = f\"\\\\textbf{{{v1:.2f}}}\"\n",
    "        else:\n",
    "            s1 = f\"{v1:.2f}\"\n",
    "\n",
    "        if np.isclose(v2, best2[idx]):\n",
    "            s2 = f\"\\\\textbf{{{v2:.2f}}}\"\n",
    "        else:\n",
    "            s2 = f\"{v2:.2f}\"\n",
    "\n",
    "        cells.append(f\"{s1}/{s2}\")\n",
    "\n",
    "    row_str = \" & \".join(cells)\n",
    "    print(f\"{i} & {row_str} \\\\\\\\\")\n",
    "\n",
    "# Add a horizontal line\n",
    "print(\"\\\\hline\")\n",
    "\n",
    "# Calculate and add the average row\n",
    "avg1 = np.nanmean(all_array1, axis=0)\n",
    "avg2 = np.nanmean(all_array2, axis=0)\n",
    "\n",
    "avg_cells = []\n",
    "for idx, task in enumerate(latex_order):\n",
    "    avg_v1 = avg1[idx]\n",
    "    avg_v2 = avg2[idx]\n",
    "    avg_cells.append(f\"{avg_v1:.2f}/{avg_v2:.2f}\")\n",
    "\n",
    "avg_row_str = \" & \".join(avg_cells)\n",
    "print(f\"Avg & {avg_row_str} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'evaluations_final/none-hetero-3B-426/20/client_0_output_metrics.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m     27\u001b[0m     results_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_output_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     task_metrics \u001b[38;5;241m=\u001b[39m print_tex_result(results_path, primary_metrics, percent_metrics)\n\u001b[1;32m     29\u001b[0m     all_rows\u001b[38;5;241m.\u001b[39mappend([task_metrics[task] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m latex_order])\n\u001b[1;32m     31\u001b[0m all_array   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_rows)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mprint_tex_result\u001b[0;34m(results_file, primary_metrics, percent_metrics)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_tex_result\u001b[39m(results_file, primary_metrics, percent_metrics):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m     task_metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/HFL/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'evaluations_final/none-hetero-3B-426/20/client_0_output_metrics.json'"
     ]
    }
   ],
   "source": [
    "results_dir = \"evaluations_final/none-hetero-3B-426/20\"\n",
    "is_global_model = False\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"accuracy\",\n",
    "    \"entailment\": \"accuracy\",\n",
    "    \"linguistic_acceptability\": \"accuracy\",\n",
    "    \"paraphrase\": \"accuracy\",\n",
    "    \"question_classification\": \"accuracy\",\n",
    "    \"structure_to_text\": \"rougeL\",\n",
    "    \"text_formatting\": \"rougeL\",\n",
    "    \"word_disambiguation\": \"accuracy\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\", \"rouge1\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "if is_global_model:\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "    latex_row = \" & \".join([f\"{metrics[task]:.2f}\" for task in latex_order]) + \" & \" + f\"{sum(metrics.values()) / len(metrics):.2f}\" + \" \\\\\\\\\"\n",
    "    print(latex_row)\n",
    "\n",
    "else:\n",
    "    all_rows = []\n",
    "    # 1) load everything\n",
    "    for i in range(8):\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        task_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        all_rows.append([task_metrics[task] for task in latex_order])\n",
    "\n",
    "    all_array   = np.array(all_rows)\n",
    "    best_values = np.nanmax(all_array, axis=0)\n",
    "    avg_values  = np.nanmean(all_array, axis=0)\n",
    "\n",
    "    # 2) print per‐client rows, bolding only the column‐wise maxima\n",
    "    for i, row_vals in enumerate(all_rows):\n",
    "        cells = []\n",
    "        for val, best in zip(row_vals, best_values):\n",
    "            if np.isclose(val, best):\n",
    "                cells.append(f\"\\\\textbf{{{val:.2f}}}\")\n",
    "            else:\n",
    "                cells.append(f\"{val:.2f}\")\n",
    "        print(f\"& {i} & \" + \" & \".join(cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 3) average row\n",
    "    print('\\\\cline{2-10}')\n",
    "    avg_cells = [f\"{v:.2f}\" for v in avg_values]\n",
    "    print(f\"& none (avg) & \" + \" & \".join(avg_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 4) best row (still bold)\n",
    "    best_cells = [f\"{v:.2f}\" for v in best_values]\n",
    "    print(f\"& none (best) & \" + \" & \".join(best_cells) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.50 & 86.50 & 78.50 & 70.00 & 91.50 & 68.32 & 96.29 & 55.75 & 77.79 \\\\\n"
     ]
    }
   ],
   "source": [
    "# for appendix A in domain\n",
    "\n",
    "results_dir = \"evaluations_final/none-homo-3B-426/20/\"\n",
    "mode = \"hetero-p\"  # Change this to \"none\", \"homo\", \"hetero-g\", \"hetero-d\" or \"hetero-p\"\n",
    "\n",
    "num_clients = 8 # Define the number of clients\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"rouge1\",\n",
    "    \"entailment\": \"rouge1\",\n",
    "    \"linguistic_acceptability\": \"rouge1\",\n",
    "    \"paraphrase\": \"rouge1\",\n",
    "    \"question_classification\": \"rouge1\",\n",
    "    \"structure_to_text\": \"rouge1\",\n",
    "    \"text_formatting\": \"rouge1\",\n",
    "    \"word_disambiguation\": \"rouge1\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rouge1\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "hetero_metrics = {}\n",
    "valid_clients_count = 0\n",
    "# Load metrics for each client, assuming client i corresponds to task i in latex_order\n",
    "# This interpretation might need adjustment based on your exact setup.\n",
    "if len(latex_order) != num_clients:\n",
    "        print(f\"Warning: Number of tasks ({len(latex_order)}) does not match number of clients ({num_clients}). Diagonal logic might be incorrect.\")\n",
    "\n",
    "for i in range(num_clients):\n",
    "    task = latex_order[i] # Task corresponding to this client index\n",
    "    results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "    try:\n",
    "        client_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        # Get the primary metric for the specific task 'task' from this client's results\n",
    "        hetero_metrics[task] = client_metrics.get(task, np.nan)\n",
    "        if not np.isnan(hetero_metrics[task]):\n",
    "                valid_clients_count += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Results file not found for client {i} at {results_path}. Setting metric for task '{task}' to NaN.\")\n",
    "        hetero_metrics[task] = np.nan\n",
    "\n",
    "if valid_clients_count == 0:\n",
    "    print(\"Error: No HetLoRA client data loaded.\")\n",
    "else:\n",
    "    # Format the 'diagonal' metrics into a single row, similar to 'homo' mode\n",
    "    row_values = [hetero_metrics.get(t, np.nan) for t in latex_order]\n",
    "    latex_row = \" & \".join([f\"{v:.2f}\" for v in row_values])\n",
    "\n",
    "    # Compute average (ignoring NaN)\n",
    "    avg = np.nanmean(row_values)\n",
    "\n",
    "    # Append average\n",
    "    latex_row += f\" & {avg:.2f} \\\\\\\\\"\n",
    "    print(latex_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.20 & 70.00 & 77.00 & 68.50 & 85.50 & 65.16 & 93.38 & 61.50 & 73.53 \\\\\n"
     ]
    }
   ],
   "source": [
    "# for appendix A\n",
    "\n",
    "results_dir = \"evaluations_final/homo-3B-r4/20\"\n",
    "is_global_model = True\n",
    "\n",
    "primary_metrics = {\n",
    "    \"coreference\": \"rouge1\",\n",
    "    \"entailment\": \"rouge1\",\n",
    "    \"linguistic_acceptability\": \"rouge1\",\n",
    "    \"paraphrase\": \"rouge1\",\n",
    "    \"question_classification\": \"rouge1\",\n",
    "    \"structure_to_text\": \"rouge1\",\n",
    "    \"text_formatting\": \"rouge1\",\n",
    "    \"word_disambiguation\": \"rouge1\"\n",
    "}\n",
    "percent_metrics = {\"accuracy\", \"f1_score\", \"rougeL\", \"rouge1\"}\n",
    "latex_order = list(primary_metrics.keys())\n",
    "\n",
    "if is_global_model:\n",
    "    results_path = os.path.join(results_dir, \"global_output_metrics.json\")\n",
    "    metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "    latex_row = \" & \".join([f\"{metrics[task]:.2f}\" for task in latex_order]) + \" & \" + f\"{sum(metrics.values()) / len(metrics):.2f}\" + \" \\\\\\\\\"\n",
    "    print(latex_row)\n",
    "\n",
    "else:\n",
    "    all_rows = []\n",
    "    # 1) load everything\n",
    "    for i in range(8):\n",
    "        results_path = os.path.join(results_dir, f\"client_{i}_output_metrics.json\")\n",
    "        task_metrics = print_tex_result(results_path, primary_metrics, percent_metrics)\n",
    "        all_rows.append([task_metrics[task] for task in latex_order])\n",
    "\n",
    "    all_array   = np.array(all_rows)\n",
    "    best_values = np.nanmax(all_array, axis=0)\n",
    "    avg_values  = np.nanmean(all_array, axis=0)\n",
    "\n",
    "    # 2) print per‐client rows, bolding only the column‐wise maxima\n",
    "    for i, row_vals in enumerate(all_rows):\n",
    "        cells = []\n",
    "        for val, best in zip(row_vals, best_values):\n",
    "            if np.isclose(val, best):\n",
    "                cells.append(f\"\\\\textbf{{{val:.2f}}}\")\n",
    "            else:\n",
    "                cells.append(f\"{val:.2f}\")\n",
    "        print(f\"& {i} & \" + \" & \".join(cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 3) average row\n",
    "    print('\\\\cline{2-10}')\n",
    "    avg_cells = [f\"{v:.2f}\" for v in avg_values]\n",
    "    avg_cells.append(f\"{sum(avg_values) / len(avg_values):.2f}\")\n",
    "    print(f\"& none (avg) & \" + \" & \".join(avg_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # 4) best row (still bold)\n",
    "    best_cells = [f\"{v:.2f}\" for v in best_values]\n",
    "    best_cells.append(f\"{sum(best_values) / len(best_values):.2f}\")\n",
    "    print(f\"& none (best) & \" + \" & \".join(best_cells) + \" \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODRrMkgL9BwBuSuCHRTfMB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
